% document class
\documentclass[titlepage]{article}

% document layout packages 
\usepackage{geometry} % allows easier page formatting
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

% formatting
\usepackage{indentfirst} % suppresses inbuilt "no-indent" after section
\usepackage{amsmath} % amsmath package
\usepackage[english]{isodate} % date package
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=red]{hyperref} % hyperlink package
\usepackage{setspace}% Using \doublespacing in the preamble 
\doublespacing % changes the text to double-line spacing


% font (inter light)
\usepackage[sfdefault,light]{inter} %% Option 'sfdefault' only if 'inter' is to be used
\usepackage[T1]{fontenc}
\usepackage{enumerate}

\begin{document} % begin document!

% title page
\title{\textbf{Module 2 Assignment}}
\author{Nathan Shea Ouedraogo}
\date{\printdateTeX{2024/03/04}}
\maketitle


% Question 1
\section{}
\setlength{\parindent}{20pt}
\begin{enumerate}[a)] % chktex 17
    \item 
    FTIR Microscopy ($\mu$-FTIR) combines microscopic methods with FTIR analysis. 
    This method relies on the transmission of visible and infrared radiation through 
    a sample material and can provide detailed information about the physical structures
    of small structures. It can provide chemical and physical properties of the materials
    (such as composition, thickness, structure) via IR and map it into a physical grid 
    across the sample. A major drawback to $\mu$-FTIR is in its spatial resolution
    (typical: $2.5\mu m - 25\mu m$) which makes it unsuitable for fine measurements requiring 
    resolutions at and below a few microns. Additionally, since water is absorbs strongly in 
    the IR spectra living systems are difficult to measure. Optical-Photothermal Infrared Spectroscopy
    (O-PTIR) is a method which can overcome these limitations. A tunable laser in the mid-IR is 
    pulsed and heats the sample of interest at a specific wavelength causing absorption of the sample. 
    A visible light probe measures the resultant photothermal effect, meaning the resultant spectra 
    are dependant on visible light and not infrared light. Additionally the spatial resolution is much 
    better allowing the probing of smaller structures. Like $\mu$-FTIR, O-PTIR uses IR to 
    probe molecular bonds and cause vibrational excitations however unlike $\mu$-FTIR the measurements 
    are dependent on visible light and not IR. Finally, $\mu$-FTIR can be used for hard and soft
    sample types whereas O-PTIR is better suited for semi-hard to soft material.
    
    \item
    Infrared nanospectroscopy combine nanometer scale spectroscopic methods with infrared spectroscopy. 
    Two popular methods are AFM-IR and IR s-SNOM. AFM-IR combines atomic force microscopy (AFM)
    with infrared spectroscopy. AFM allows high resolution scanning of the surfaces of 
    structures. A cantilever is run across a the surface and any defects cause the cantilever 
    to riser or fall. This causes a change in measured potential which can be used to map the 
    physical surface of a material. In AFM-IR, the sample material is irradiated with infrared 
    radiation from a tunable laser causing absorption in the sample. The resultant expansion 
    caused by the photothermal absorption into the sample is transduced by the cantilever tip. 
    This allows for high resolution topographical mapping of IR information. IR s-SNOM (scattering-type Scanning Near-field Optical Microscopy) uses an AFM cantilever to provide topographical 
    mapping of a surface. Unlike AFM-IR which transduces the displacement of the cantilever 
    due to photothermal expansion, IR s-SNOM detects the \emph{scattering} of IR due to 
    the cantilever tip. In AFM-IR, the sample is irradiated whereas in IR s-SNOM the cantilever is 
    irradiated and the scattering caused by the tip is related to the absorption 
    coefficient of the material. AFM-IR is best suited for soft material 
    as they have large thermal expansion coefficients whereas s-sNOM is 
    better suited for hard materials which scatter light well.
    
    \item
    There are two main methods of implementing machine learning; supervised and unsupervised 
    methods. In supervised learning, data is labelled before being used in training. 
    This means that the algorithm has a baseline `knowledge' of what the correct output 
    should be and what relations to look for. It is more time consuming as the data must be conditioned
    before being fed to the algorithm, cannot resolve latent variables, and is generally 
    more accurate than unsupervised methods. Unsupervised methods use unlabeled data 
    meaning the algorithm has no advanced knowledge of what relationships should be present in the 
    data. This method is very powerful for mining large amounts of data with latent variables at the cost
    of accuracy and resource use. Accuracy may suffer (especially in early stages of learning) as these algorithms
    must be trained to output nonspurious relations within the data. This also results in a higher overall 
    use of resources and much greater complexity. 
\end{enumerate}

% Question 2
\section{}
\setlength{\parindent}{20pt}
\par \emph{k}-Means is a type of unsupervised machine learning used in data analysis. 
It takes a set of data points and divides them into a number of clusters 
determined by the user. Each cluster is averaged and the outputted data is 
reduced in size and organized by correlation. Centroids are the arithmetic center 
of a given group of data points. Data points are assigned to clusters based on 
how close they are to any \emph{k} $>$ 1 clusters. The underlying assumption is that closely spaced 
data points are correlated and represent the same variable. \emph{k}-Means works well for unlabeled data 
with a known number of categories. For example this method would work well for combined spectroscopic 
and visual data where absorption bands for some feature is known but the raw spectral data is unlabeled. 
Using \emph{k}-means one could set the number of clusters to the number of expected features and average the data 
into clusters corresponding to these features. 

% Question 3
\section{}
\setlength{\parindent}{20pt}
\par Principle component analysis (PCA) is a method of reducing the amount of axis of a given data set with the goal of finding new axis (`components') around which the dataset has high variance. It does this by trying to maximize the variance along a new linear fitting curve and divide the data along these new relations. PCA only works for normally distributed data which explains part of the reason why it can help reduce noise. PCA fits the data along n-axis and ranks them according to how strongly these variables contribute to variation in the data. Given a base of good raw signal to noise ratio (SNR) in the data, it can generally be assumed that the first principal component correspond to data, as they should be the greatest contributors to the observed variance and noise increases with higher order axes. Therefore assuming good SNR, noise should not be a major contributor of the observed variance in the signal along its most contributing PC axis. Noise can thus be greatly reduced as axis which do not strongly contribute to the data are eliminated which has the overall effect of eliminating their contribution of noise. 

% Question 4
\section{}
\setlength{\parindent}{20pt}
\par The internal workings of an FT-IR spectrometer contain a interferometer which detects the interference patter caused by two beams of light. In the most basic version of a Michelson interferometer (which will be used in the following for simplicity), a coherent light source is directed at a beam splitter which directs the light to two mirrors. The first mirror is static at a fixed distance from the beam splitter and the second mirror moves back and forth, towards and away the beam splitter. The split beams recombine at the output and the resultant interference pattern is interpreted by the instrument. As the second mirror is moved not only in space but also in time, the interference patterns will change in relation to the position of the movable mirror as a function of time. By applying a Fast-Fourier Transform (FFT) to the interferogram, the signal can be translated from the time domain to the frequency domain. Two values relating the distance of the mirrors to the beam splitter are important. The optical path difference (OPD) is the difference in length from the beam splitter to the static mirror and the beam splitter and the moving mirror, which is obviously related to the position of the moving mirror. The zero path difference is the difference between the two paths when the moving mirror is at its `zero' position (ideally should be zero). 
\par Knowing the exact OPD values are critical for deriving accurate FFTs since the FFT is the inverse of the OPD value. The change in OPD ($\Delta$OPD) must be precisely known as a change in wavelength maxima will cause artifacts in the interferometer. Controlling the velocity of the mirror drive to the extremely high levels of accuracy needed is incredibly costly and using a single wavelength would make the spectrographer useless. Instead, a reference beam with a very high level of coherence across the optical path (ie the wavelength must be temporally and spectrally coherent) is shone parallel to the sample beam and collected in a separate detector. This reference inteferometer is used to correct for the inaccuracy in ZPD and mirror velocity allowing accurate and high resolution $\Delta$OPD readings. He-Ne beams are incredibly popular in many optical applications due to their reliability, stability, and ease of use.
\par Diode lasers may be used instead of a He-Ne laser, and offer several potential advantages; they are cheaper to produce, last much longer as they do not degrade like gas lasers, have tunable emission ranges, are resistant to ambient interference, and are much less bulky. However, they offer shorter coherence lengths, have a more broadband emission spectra, and are very prone to temperature fluctuations.

\par Exchanging the reference beam from 633nm to 532nm would theoretically produce a better Nyquist frequency. The Nyquist frequency is defined as the smallest frequency which can be measured accuratley and is calculated as 2x the sampling frequency. In theory the 532nm diode laser would offer a better resolution as compared to a 633nm gass beam. \\ \\ \\ % chktex 13

\begin{small}
    Sources:
    \begin{enumerate}
        \item  \url{https://www.newport.com/n/introduction-to-ftir-spectroscopy}
        \item \url{https://www.shimadzu.eu/sites/shimadzu.seg/files/ftir_talkletter_vol_24.pdf}
    \end{enumerate}
\end{small}
% [chktex]-file 17
% Question 4
\newpage
\section{}
\setlength{\parindent}{20pt} 
\par $\mu$-FTIR, PTIR, and O-PTIR were used to analyze two different cross-sectional sample thicknesses of $\approx$6 $\mu$m for $\mu$-FTIR, and $\approx$400 $n$m for PTIR and O-PTIR, with analyzed areas of $\approx$2$\times$10\textsuperscript{11}nm\textsuperscript{3} for $\mu$-FTIR, $\approx$10\textsuperscript{8}nm\textsuperscript{3} for O-PTIR, and $\approx$5$\times$10\textsuperscript{3}nm\textsuperscript{3} for PTIR. These differences in areas allowed the authors to resolve different metal carboxylate species present in the layers of the paint in low concentrations.The spatial resolution of O-PTIR ($\approx$500nm) allowed the authors to distinguish species with low concentrations and phase separated in the paint surface whereas the spatial resolution of PTIR ($\approx$10nm) allowed the authors to map the distribution of chemical species at the nanoscale. 
\par Each method required different methods of data acquisition. $\mu$-FTIR samples were placed between KBr pellets and analyzed using  a synchrotron beamline at a frequency range of 7500 to 750cm.\textsuperscript{-1} and the absorption bands were integrated and mapped. The OPT-IR instrument equipped with a quantum cascade laser (920-1800cm\textsuperscript{-1}  and 2660-3020cm\textsuperscript{-1}) and a visible light laser (532nm). Finally, PTIR was preformed with a sweep of 1300 to 1900cm\textsuperscript{-1} and an IR laser focused on the sample. A Savitzky-Golay filter was used to smooth the data and the data was normalized. 
\par Different analytical techniques may have been employed to analyze the surface deposition. NMR may have been used to help identify different carboxylate species. Mass-spectroscopy imaging could be useful to determine the chemical composition of the surface layer and map it into a 3-D field. The most useful method (and likely the most destructive) would be to use a GC-MS on the samples. This would allow characterization of the chemical composition of the sample with a very high degree of specie resolution.
\par There are many research problems which could benefit from a multiscale approach like the one used in this paper. The paper used a multiscale approach to probe different scales on the surface of the paint film. This allowed the resolvement and characterization of different metal complexes at varying concentrations. Extending this method to thin films on solid surfaces (such as urban grime, bacterial mats, dust, etc\dots) could allow researchers to probe the composition and spatial arrangement of these materials. 
\end{document}